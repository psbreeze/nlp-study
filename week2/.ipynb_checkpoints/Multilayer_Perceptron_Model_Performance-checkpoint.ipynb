{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list='2'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 6s 3us/step\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 1s 3us/step: \n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "\n",
    "#1. 데이터셋 준비\n",
    "max_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lt dlrs demand 000 reuter dividend year lt plus billion 04 000 reuter dividend year an worth new vs reuter dlrs of on shrs earnings countries new vs reuter 1985 billion vs 2 lt 4 division 000 reuter from go 000 lt plus which of 000 reuter from total 000 an 71 billion vs reuter dlr also vs shrs earnings countries 4 vs reuter 1985 from vs some now april 0 related in corp it inc strong cents dollar were after april 0 of or of more index 10 of company taking report it in estimated but trading texas said united said of a of up said countries vs 000 3 of central said which of on future of said of a includes of profit said meeting trade vs 3 of up said 1985 were vs pct dlrs\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "## 데이터 보기\n",
    "index_to_word = {}\n",
    "for key, value in word_index.items():\n",
    "    index_to_word[value] = key\n",
    "print(' '.join([index_to_word[x] for x in x_train[2]]))\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    " # for convering sentences to matrix\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "#Convert class vector to binary class matrix for use with categorical_crossentropy\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/40\n",
      "8083/8083 [==============================] - 3s 416us/step - loss: 3.3705 - acc: 0.3302 - val_loss: 2.9083 - val_acc: 0.3315\n",
      "Epoch 2/40\n",
      "8083/8083 [==============================] - 2s 240us/step - loss: 2.7355 - acc: 0.3504 - val_loss: 2.6054 - val_acc: 0.3315\n",
      "Epoch 3/40\n",
      "8083/8083 [==============================] - 2s 236us/step - loss: 2.5493 - acc: 0.3444 - val_loss: 2.4840 - val_acc: 0.3315\n",
      "Epoch 4/40\n",
      "8083/8083 [==============================] - 2s 236us/step - loss: 2.4301 - acc: 0.3462 - val_loss: 2.3276 - val_acc: 0.3315\n",
      "Epoch 5/40\n",
      "8083/8083 [==============================] - 2s 251us/step - loss: 2.3437 - acc: 0.3493 - val_loss: 2.2081 - val_acc: 0.3315\n",
      "Epoch 6/40\n",
      "8083/8083 [==============================] - 2s 258us/step - loss: 2.2689 - acc: 0.3829 - val_loss: 2.1532 - val_acc: 0.4650\n",
      "Epoch 7/40\n",
      "8083/8083 [==============================] - 3s 339us/step - loss: 2.2227 - acc: 0.4196 - val_loss: 2.1324 - val_acc: 0.4894\n",
      "Epoch 8/40\n",
      "8083/8083 [==============================] - 2s 269us/step - loss: 2.1878 - acc: 0.4368 - val_loss: 2.0699 - val_acc: 0.4983\n",
      "Epoch 9/40\n",
      "8083/8083 [==============================] - 3s 363us/step - loss: 2.1554 - acc: 0.4425 - val_loss: 2.0665 - val_acc: 0.4994\n",
      "Epoch 10/40\n",
      "8083/8083 [==============================] - 2s 294us/step - loss: 2.1418 - acc: 0.4458 - val_loss: 2.0770 - val_acc: 0.4983\n",
      "Epoch 11/40\n",
      "8083/8083 [==============================] - 2s 278us/step - loss: 2.1219 - acc: 0.4527 - val_loss: 2.0703 - val_acc: 0.4816\n",
      "Epoch 12/40\n",
      "8083/8083 [==============================] - 2s 271us/step - loss: 2.1160 - acc: 0.4441 - val_loss: 2.0485 - val_acc: 0.4839\n",
      "Epoch 13/40\n",
      "8083/8083 [==============================] - 2s 282us/step - loss: 2.0847 - acc: 0.4522 - val_loss: 2.0335 - val_acc: 0.4994\n",
      "Epoch 14/40\n",
      "8083/8083 [==============================] - 2s 288us/step - loss: 2.0597 - acc: 0.4592 - val_loss: 2.0424 - val_acc: 0.5050\n",
      "Epoch 15/40\n",
      "8083/8083 [==============================] - 3s 334us/step - loss: 2.0327 - acc: 0.4668 - val_loss: 2.0280 - val_acc: 0.5083\n",
      "Epoch 16/40\n",
      "8083/8083 [==============================] - 2s 302us/step - loss: 2.0244 - acc: 0.4658 - val_loss: 2.0325 - val_acc: 0.4994\n",
      "Epoch 17/40\n",
      "8083/8083 [==============================] - 3s 361us/step - loss: 2.0365 - acc: 0.4653 - val_loss: 1.9716 - val_acc: 0.5039\n",
      "Epoch 18/40\n",
      "8083/8083 [==============================] - 2s 298us/step - loss: 2.0199 - acc: 0.4680 - val_loss: 1.9906 - val_acc: 0.5050\n",
      "Epoch 19/40\n",
      "8083/8083 [==============================] - 2s 309us/step - loss: 1.9993 - acc: 0.4652 - val_loss: 1.9841 - val_acc: 0.5083\n",
      "Epoch 20/40\n",
      "8083/8083 [==============================] - 2s 303us/step - loss: 1.9976 - acc: 0.4691 - val_loss: 1.9840 - val_acc: 0.5050\n",
      "Epoch 21/40\n",
      "8083/8083 [==============================] - 3s 315us/step - loss: 1.9901 - acc: 0.4711 - val_loss: 1.9761 - val_acc: 0.5072\n",
      "Epoch 22/40\n",
      "8083/8083 [==============================] - 3s 360us/step - loss: 1.9921 - acc: 0.4691 - val_loss: 1.9619 - val_acc: 0.5050\n",
      "Epoch 23/40\n",
      "8083/8083 [==============================] - 3s 373us/step - loss: 1.9721 - acc: 0.4719 - val_loss: 2.0034 - val_acc: 0.5083\n",
      "Epoch 24/40\n",
      "8083/8083 [==============================] - 3s 392us/step - loss: 1.9676 - acc: 0.4796 - val_loss: 1.9886 - val_acc: 0.5072\n",
      "Epoch 25/40\n",
      "8083/8083 [==============================] - 3s 379us/step - loss: 1.9604 - acc: 0.4749 - val_loss: 1.9732 - val_acc: 0.5017\n",
      "Epoch 26/40\n",
      "8083/8083 [==============================] - 4s 470us/step - loss: 1.9614 - acc: 0.4784 - val_loss: 1.9864 - val_acc: 0.5072\n",
      "Epoch 27/40\n",
      "8083/8083 [==============================] - 3s 383us/step - loss: 1.9563 - acc: 0.4799 - val_loss: 1.9861 - val_acc: 0.4872\n",
      "Epoch 28/40\n",
      "8083/8083 [==============================] - 3s 373us/step - loss: 1.9558 - acc: 0.4818 - val_loss: 1.9822 - val_acc: 0.5106\n",
      "Epoch 29/40\n",
      "8083/8083 [==============================] - 3s 377us/step - loss: 1.9365 - acc: 0.4850 - val_loss: 1.9728 - val_acc: 0.5061\n",
      "Epoch 30/40\n",
      "8083/8083 [==============================] - 3s 350us/step - loss: 1.9428 - acc: 0.4826 - val_loss: 1.9812 - val_acc: 0.4950\n",
      "Epoch 31/40\n",
      "1440/8083 [====>.........................] - ETA: 2s - loss: 1.9310 - acc: 0.4972"
     ]
    }
   ],
   "source": [
    "#2. model design for relu activation \n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.noise import AlphaDropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(1000,),\n",
    "                    kernel_initializer='glorot_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "for i in range(5):\n",
    "    model.add(Dense(16, kernel_initializer='glorot_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(46))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history_model_relu = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=16,\n",
    "                            epochs=40,\n",
    "                            verbose=1,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "score_model_relu = model.evaluate(x_test,\n",
    "                               y_test,\n",
    "                               batch_size=16,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(1000,),\n",
    "                    kernel_initializer='lecun_normal'))\n",
    "model.add(Activation('selu'))\n",
    "model.add(AlphaDropout(0.1))\n",
    "\n",
    "for i in range(5):\n",
    "    model.add(Dense(16, kernel_initializer='lecun_normal'))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(AlphaDropout(0.1))\n",
    "\n",
    "model.add(Dense(46))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history_model_selu = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=16,\n",
    "                            epochs=40,\n",
    "                            verbose=1,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "score_model_selu = model.evaluate(x_test,\n",
    "                               y_test,\n",
    "                               batch_size=16,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('relu result')\n",
    "print('Test score:', score_model_relu[0])\n",
    "print('Test accuracy:', score_model_relu[1])\n",
    "print('selu result')\n",
    "print('Test score:', score_model_selu[0])\n",
    "print('Test accuracy:', score_model_selu[1])\n",
    "\n",
    "epochs=40\n",
    "plt.figure()\n",
    "plt.plot(range(epochs),\n",
    "         history_model_relu.history['val_loss'],\n",
    "         'g-',\n",
    "         label='relu Val Loss')\n",
    "plt.plot(range(epochs),\n",
    "         history_model_selu.history['val_loss'],\n",
    "         'r-',\n",
    "         label='selu Val Loss')\n",
    "plt.plot(range(epochs),\n",
    "         history_model_relu.history['loss'],\n",
    "         'g--',\n",
    "         label='relu Loss')\n",
    "plt.plot(range(epochs),\n",
    "         history_model_selu.history['loss'],\n",
    "         'r--',\n",
    "         label='selu Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
